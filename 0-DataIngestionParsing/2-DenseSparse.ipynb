{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b0acc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications using modular components\"),\n",
    "    Document(page_content=\"LangChain provides tools for building applications powered by large language models\"),\n",
    "    Document(page_content=\"You can build LLM-based applications using LangChain and vector databases\"),\n",
    "    \n",
    "    Document(page_content=\"Pinecone is a managed vector database for similarity search\"),\n",
    "    Document(page_content=\"Vector databases like Pinecone and Chroma are used for semantic search\"),\n",
    "    Document(page_content=\"Chroma is an open-source vector store for embeddings\"),\n",
    "    \n",
    "    Document(page_content=\"Hybrid search combines dense embeddings with sparse keyword matching\"),\n",
    "    Document(page_content=\"Dense retrieval captures semantic meaning using vector embeddings\"),\n",
    "    Document(page_content=\"Sparse retrieval uses TF-IDF or BM25 for exact keyword search\"),\n",
    "    \n",
    "    Document(page_content=\"BM25 is a ranking function widely used in information retrieval\"),\n",
    "    Document(page_content=\"TF-IDF converts text into sparse vectors based on term frequency\"),\n",
    "    \n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris, France\"),\n",
    "    Document(page_content=\"Paris is the capital city of France\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9408b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93af5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma \n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48a13f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Results:\n",
      "\n",
      "0.4040-->You can build LLM-based applications using LangChain and vector databases\n",
      "0.4040-->You can build LLM-based applications using LangChain and vector databases\n",
      "0.4040-->You can build LLM-based applications using LangChain and vector databases\n",
      "0.4040-->You can build LLM-based applications using LangChain and vector databases\n",
      "0.9638-->Vector databases like Pinecone and Chroma are used for semantic search\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query=\"build LLM applications using vector databases\"\n",
    "dense_results=vectorstore.similarity_search_with_score(query,k=5)\n",
    "\n",
    "print(\"Dense Results:\\n\")\n",
    "\n",
    "for doc,distance in dense_results:\n",
    "    print(f\"{distance:.4f}-->{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fb10095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparse Result:\n",
      "\n",
      "LangChain helps build LLM applications using modular components\n",
      "You can build LLM-based applications using LangChain and vector databases\n",
      "Dense retrieval captures semantic meaning using vector embeddings\n",
      "Vector databases like Pinecone and Chroma are used for semantic search\n",
      "LangChain provides tools for building applications powered by large language models\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25=BM25Retriever.from_documents(docs)\n",
    "bm25.k=5\n",
    "\n",
    "sparse_result=bm25.invoke(query)\n",
    "\n",
    "print(\"\\nSparse Result:\\n\")\n",
    "for doc in sparse_result:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65465c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  # weight for dense retrieval\n",
    "\n",
    "scores = {}\n",
    "\n",
    "# Dense scores\n",
    "for doc, distance in dense_results:\n",
    "    dense_score = 1 / (1 + distance)  # convert distance → similarity\n",
    "    scores[doc.page_content] = alpha * dense_score\n",
    "\n",
    "# Sparse scores (rank-based)\n",
    "for rank, doc in enumerate(sparse_result):\n",
    "    sparse_score = 1 / (rank + 1)\n",
    "    scores[doc.page_content] = scores.get(doc.page_content, 0) + (1 - alpha) * sparse_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "440e9dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Hybrid Ranking:\n",
      "\n",
      "0.606 → You can build LLM-based applications using LangChain and vector databases\n",
      "0.500 → LangChain helps build LLM applications using modular components\n",
      "0.380 → Vector databases like Pinecone and Chroma are used for semantic search\n",
      "0.167 → Dense retrieval captures semantic meaning using vector embeddings\n",
      "0.100 → LangChain provides tools for building applications powered by large language models\n"
     ]
    }
   ],
   "source": [
    "hybrid_results=sorted(\n",
    "    scores.items(),\n",
    "    key=lambda x:x[1],\n",
    "    \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Hybrid Ranking:\\n\")\n",
    "for text, score in hybrid_results:\n",
    "    print(f\"{score:.3f} → {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f72b4873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can build LLM-based applications using LangChain and vector databases\\nLangChain helps build LLM applications using modular components\\nVector databases like Pinecone and Chroma are used for semantic search'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k = 3\n",
    "\n",
    "retrieved_context = \"\\n\".join(\n",
    "    [text for text, _ in hybrid_results[:top_k]]\n",
    ")\n",
    "\n",
    "retrieved_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0177ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an intelligent assistant.\n",
    "\n",
    "Use the following retrieved context to answer the question.\n",
    "If the answer is not present in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44ccc311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an intelligent assistant.\n",
      "\n",
      "Use the following retrieved context to answer the question.\n",
      "If the answer is not present in the context, say \"I don't know\".\n",
      "\n",
      "Context:\n",
      "You can build LLM-based applications using LangChain and vector databases\n",
      "LangChain helps build LLM applications using modular components\n",
      "Vector databases like Pinecone and Chroma are used for semantic search\n",
      "\n",
      "Question:\n",
      "build LLM applications using vector databases\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_prompt = rag_prompt.format(\n",
    "    context=retrieved_context,\n",
    "    question=query\n",
    ")\n",
    "\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "990c9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",\n",
    "    max_new_tokens=200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d97cef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      "I don't know\n"
     ]
    }
   ],
   "source": [
    "response = llm(final_prompt)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_COURSE (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
