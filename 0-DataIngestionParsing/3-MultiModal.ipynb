{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e493ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for page in doc:\n",
    "        # Extract text\n",
    "        text = page.get_text()\n",
    "        if text.strip():\n",
    "            texts.append(text)\n",
    "\n",
    "        # Extract images\n",
    "        for img in page.get_images(full=True):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "\n",
    "    return texts, images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8096035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def chunk_text(texts):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for text in texts:\n",
    "        chunks = splitter.split_text(text)\n",
    "        for chunk in chunks:\n",
    "            documents.append(Document(page_content=chunk))\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e482e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9085561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(texts):\n",
    "    inputs = clip_processor(\n",
    "        text=texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.get_text_features(**inputs)\n",
    "\n",
    "    return embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "686994dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_images(images):\n",
    "    if len(images) == 0:\n",
    "        print(\"⚠️ No images found in document.\")\n",
    "        return np.empty((0, 512))  # CLIP image embedding size\n",
    "\n",
    "    inputs = clip_processor(\n",
    "        images=images,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.get_image_features(**inputs)\n",
    "\n",
    "    return embeddings.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a078db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "def build_vectorstore(documents, embeddings):\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    ids = [f\"doc_{i}\" for i in range(len(texts))]\n",
    "\n",
    "    client = chromadb.Client()\n",
    "\n",
    "    # Delete if exists (safe)\n",
    "    try:\n",
    "        client.delete_collection(\"multimodal_text\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = client.create_collection(name=\"multimodal_text\")\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=texts,\n",
    "        embeddings=embeddings.tolist()\n",
    "    )\n",
    "\n",
    "    vectorstore = Chroma(\n",
    "        client=client,\n",
    "        collection_name=\"multimodal_text\",\n",
    "        embedding_function=None\n",
    "    )\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "04323ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def retrieve(query, vectorstore, image_embeddings, images, top_k=3):\n",
    "    # Embed query\n",
    "    query_emb = embed_texts([query])\n",
    "\n",
    "    # -------- TEXT RETRIEVAL --------\n",
    "    results = vectorstore._collection.get(include=[\"embeddings\", \"documents\"])\n",
    "    text_embs = np.array(results[\"embeddings\"])\n",
    "\n",
    "    text_scores = cosine_similarity(query_emb, text_embs)[0]\n",
    "    top_text_idx = text_scores.argsort()[-top_k:][::-1]\n",
    "    retrieved_texts = [results[\"documents\"][i] for i in top_text_idx]\n",
    "\n",
    "    # -------- IMAGE RETRIEVAL (SAFE) --------\n",
    "    retrieved_images = []\n",
    "\n",
    "    if image_embeddings is not None and len(image_embeddings) > 0:\n",
    "        image_scores = cosine_similarity(query_emb, image_embeddings)[0]\n",
    "        top_image_idx = image_scores.argsort()[-top_k:][::-1]\n",
    "        retrieved_images = [images[i] for i in top_image_idx]\n",
    "    else:\n",
    "        print(\"ℹ️ No images available for retrieval.\")\n",
    "\n",
    "    return retrieved_texts, retrieved_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3026c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-small\",  # smaller, less storage\n",
    "    max_new_tokens=150\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are a helpful AI assistant.\n",
    "\n",
    "Use ONLY the information in the context to answer the question.\n",
    "If the answer is not present, say \"Not found in the document\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in 2–3 complete sentences:\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95052c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No images found in document.\n",
      "ℹ️ No images available for retrieval.\n",
      "\n",
      "✅ Final Answer:\n",
      "\n",
      "Inspired by the structure of the human brain. They consist of interconnected nodes called neurons, organized into layers: input layer, hidden layers, and output layer. Each neuron performs a weighted sum of inputs, adds a bias, and applies an activation function. Training a neural network involves adjusting weights using optimization algorithms such as Gradient Descent and Backpropagation. Deep Learning (DL) and reinforcement learning. Supervised learning uses labeled data, unsupervised learning works with unlabeled Neural Networks are inspired by the structure of the human brain. They consist of interconnected nodes called neurons, organized into layers: input layer, hidden layers, and output\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"pdf/p1.pdf\"\n",
    "texts, images = load_pdf(pdf_path)\n",
    "\n",
    "documents = chunk_text(texts)\n",
    "\n",
    "text_embeddings = embed_texts([doc.page_content for doc in documents])\n",
    "image_embeddings = embed_images(images)\n",
    "\n",
    "vectorstore = build_vectorstore(documents, text_embeddings)\n",
    "\n",
    "query =\"What are Neural Networks\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "retrieved_texts, retrieved_images = retrieve(\n",
    "    query,\n",
    "    vectorstore,\n",
    "    image_embeddings,\n",
    "    images\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "context = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "final_prompt = prompt.format(\n",
    "    \n",
    "    context=context,\n",
    "    question=query\n",
    ")\n",
    "\n",
    "response = llm.invoke(final_prompt)\n",
    "\n",
    "print(\"\\n✅ Final Answer:\\n\")\n",
    "print(response.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e6475b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for img in retrieved_images:\n",
    "    display(img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_COURSE (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
